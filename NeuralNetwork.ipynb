{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting library\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Optimization module in scipy\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    z = np.array(z)\n",
    "    g = np.zeros(z.shape)\n",
    "    g = 1 / (1 + np.exp(-z))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "\n",
    "\n",
    "    g = np.zeros(z.shape)\n",
    "    g = sigmoid(z) * (1 - sigmoid(z))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnCostFunction(nn_params,\n",
    "                   input_layer_size,\n",
    "                   hidden_layer_size,\n",
    "                   num_labels,\n",
    "                   X, y, lambda_=0.0):\n",
    "\n",
    "    Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                        (hidden_layer_size, (input_layer_size + 1)))\n",
    "\n",
    "    Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                        (num_labels, (hidden_layer_size + 1)))\n",
    "    \n",
    "    m = y.size\n",
    "\n",
    "    J = 0\n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "\n",
    "    a1 = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
    "    \n",
    "    a2 = sigmoid(a1.dot(Theta1.T))\n",
    "    a2 = np.concatenate([np.ones((a2.shape[0], 1)), a2], axis=1)\n",
    "    \n",
    "    a3 = sigmoid(a2.dot(Theta2.T))\n",
    "    \n",
    "    y_matrix = y.reshape(-1)\n",
    "    y_matrix = np.eye(num_labels)[y_matrix]\n",
    "\n",
    "    delta_3 = a3 - y_matrix\n",
    "    delta_2 = delta_3.dot(Theta2)[:, 1:] * sigmoidGradient(a1.dot(Theta1.T))\n",
    "\n",
    "    Delta1 = delta_2.T.dot(a1)\n",
    "    Delta2 = delta_3.T.dot(a2)\n",
    "    \n",
    "    # Add regularization to gradient\n",
    "\n",
    "    Theta1_grad = (1 / m) * Delta1\n",
    "    Theta1_grad[:, 1:] = Theta1_grad[:, 1:] + (lambda_ / m) * Theta1[:, 1:]\n",
    "    \n",
    "    Theta2_grad = (1 / m) * Delta2\n",
    "    Theta2_grad[:, 1:] = Theta2_grad[:, 1:] + (lambda_ / m) * Theta2[:, 1:]\n",
    "    \n",
    "    grad = np.concatenate([Theta1_grad.ravel(), Theta2_grad.ravel()])\n",
    "    \n",
    "    return J, grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Past_International_Matches.csv\")\n",
    "X = data.loc[:, ['mean_coeff', 'importance', 'tvalue_home', 'tvalue_away']]\n",
    "\n",
    "data['is_won'] = (data['home_team_result'] == 'Win') * 1\n",
    "data['is_draw'] = (data['home_team_result'] == 'Draw') * 1\n",
    "data['is_loss'] = (data['home_team_result'] == 'Lose') * 1\n",
    "data['result'] = data['is_won'] * 1 + data['is_draw'] * 2 + data['is_loss'] * 3\n",
    "\n",
    "y = data['result']\n",
    "\n",
    "input_layer_size  = 4\n",
    "hidden_layer_size = 25\n",
    "num_labels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randInitializeWeights(L_in, L_out, epsilon_init=0.12):\n",
    "    W = np.zeros((L_out, 1 + L_in))\n",
    "    W = np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n",
    "\n",
    "# Unroll parameters\n",
    "initial_nn_params = np.concatenate([initial_Theta1.ravel(), initial_Theta2.ravel()], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [22], line 15\u001b[0m\n\u001b[0;32m      9\u001b[0m costFunction \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m p: nnCostFunction(p, input_layer_size,\n\u001b[0;32m     10\u001b[0m                                         hidden_layer_size,\n\u001b[0;32m     11\u001b[0m                                         num_labels, X, y, lambda_)\n\u001b[0;32m     13\u001b[0m \u001b[39m# Now, costFunction is a function that takes in only one argument\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# (the neural network parameters)\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m res \u001b[39m=\u001b[39m optimize\u001b[39m.\u001b[39;49mminimize(costFunction,\n\u001b[0;32m     16\u001b[0m                         initial_nn_params,\n\u001b[0;32m     17\u001b[0m                         jac\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     18\u001b[0m                         method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mTNC\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     19\u001b[0m                         options\u001b[39m=\u001b[39;49moptions)\n\u001b[0;32m     21\u001b[0m \u001b[39m# get the solution of the optimization\u001b[39;00m\n\u001b[0;32m     22\u001b[0m nn_params \u001b[39m=\u001b[39m res\u001b[39m.\u001b[39mx\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scipy\\optimize\\_minimize.py:702\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    699\u001b[0m     res \u001b[39m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    700\u001b[0m                            callback\u001b[39m=\u001b[39mcallback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    701\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtnc\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 702\u001b[0m     res \u001b[39m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    703\u001b[0m                         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    704\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcobyla\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    705\u001b[0m     res \u001b[39m=\u001b[39m _minimize_cobyla(fun, x0, args, constraints, callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    706\u001b[0m                             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scipy\\optimize\\_tnc.py:380\u001b[0m, in \u001b[0;36m_minimize_tnc\u001b[1;34m(fun, x0, args, jac, bounds, eps, scale, offset, mesg_num, maxCGit, maxiter, eta, stepmx, accuracy, minfev, ftol, xtol, gtol, rescale, disp, callback, finite_diff_rel_step, maxfun, **unknown_options)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    378\u001b[0m     messages \u001b[39m=\u001b[39m MSG_NONE\n\u001b[1;32m--> 380\u001b[0m sf \u001b[39m=\u001b[39m _prepare_scalar_function(fun, x0, jac\u001b[39m=\u001b[39;49mjac, args\u001b[39m=\u001b[39;49margs, epsilon\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    381\u001b[0m                               finite_diff_rel_step\u001b[39m=\u001b[39;49mfinite_diff_rel_step,\n\u001b[0;32m    382\u001b[0m                               bounds\u001b[39m=\u001b[39;49mnew_bounds)\n\u001b[0;32m    383\u001b[0m func_and_grad \u001b[39m=\u001b[39m sf\u001b[39m.\u001b[39mfun_and_grad\n\u001b[0;32m    385\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \u001b[39mlow, up   : the bounds (lists of floats)\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \u001b[39m            if low is None, the lower bounds are removed.\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[39m            if up is None, the upper bounds are removed.\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[39m            low and up defaults to None\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scipy\\optimize\\_optimize.py:263\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[1;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[0;32m    259\u001b[0m     bounds \u001b[39m=\u001b[39m (\u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf, np\u001b[39m.\u001b[39minf)\n\u001b[0;32m    261\u001b[0m \u001b[39m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[39m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[1;32m--> 263\u001b[0m sf \u001b[39m=\u001b[39m ScalarFunction(fun, x0, args, grad, hess,\n\u001b[0;32m    264\u001b[0m                     finite_diff_rel_step, bounds, epsilon\u001b[39m=\u001b[39;49mepsilon)\n\u001b[0;32m    266\u001b[0m \u001b[39mreturn\u001b[39;00m sf\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scipy\\optimize\\_differentiable_functions.py:158\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[1;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx)\n\u001b[0;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fun_impl \u001b[39m=\u001b[39m update_fun\n\u001b[1;32m--> 158\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun()\n\u001b[0;32m    160\u001b[0m \u001b[39m# Gradient evaluation\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39mif\u001b[39;00m callable(grad):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_fun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun_impl()\n\u001b[0;32m    252\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scipy\\optimize\\_optimize.py:76\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39margs):\n\u001b[0;32m     75\u001b[0m     \u001b[39m\"\"\" returns the the function value \"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_if_needed(x, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scipy\\optimize\\_optimize.py:70\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mall(x \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(x)\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m---> 70\u001b[0m     fg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfun(x, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     71\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39m=\u001b[39m fg[\u001b[39m1\u001b[39m]\n\u001b[0;32m     72\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39m=\u001b[39m fg[\u001b[39m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn [22], line 9\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m      6\u001b[0m lambda_ \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[39m# Create \"short hand\" for the cost function to be minimized\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m costFunction \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m p: nnCostFunction(p, input_layer_size,\n\u001b[0;32m     10\u001b[0m                                         hidden_layer_size,\n\u001b[0;32m     11\u001b[0m                                         num_labels, X, y, lambda_)\n\u001b[0;32m     13\u001b[0m \u001b[39m# Now, costFunction is a function that takes in only one argument\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# (the neural network parameters)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m res \u001b[39m=\u001b[39m optimize\u001b[39m.\u001b[39mminimize(costFunction,\n\u001b[0;32m     16\u001b[0m                         initial_nn_params,\n\u001b[0;32m     17\u001b[0m                         jac\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     18\u001b[0m                         method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTNC\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     19\u001b[0m                         options\u001b[39m=\u001b[39moptions)\n",
      "Cell \u001b[1;32mIn [18], line 26\u001b[0m, in \u001b[0;36mnnCostFunction\u001b[1;34m(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_)\u001b[0m\n\u001b[0;32m     22\u001b[0m a2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([np\u001b[39m.\u001b[39mones((a2\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m)), a2], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m a3 \u001b[39m=\u001b[39m sigmoid(a2\u001b[39m.\u001b[39mdot(Theta2\u001b[39m.\u001b[39mT))\n\u001b[1;32m---> 26\u001b[0m y_matrix \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     27\u001b[0m y_matrix \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39meye(num_labels)[y_matrix]\n\u001b[0;32m     29\u001b[0m delta_3 \u001b[39m=\u001b[39m a3 \u001b[39m-\u001b[39m y_matrix\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\generic.py:5902\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5895\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   5896\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[0;32m   5897\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[0;32m   5898\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[0;32m   5899\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5900\u001b[0m ):\n\u001b[0;32m   5901\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[1;32m-> 5902\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "#  After you have completed the assignment, change the maxiter to a larger\n",
    "#  value to see how more training helps.\n",
    "options= {'maxiter': 100}\n",
    "\n",
    "#  You should also try different values of lambda\n",
    "lambda_ = 1\n",
    "\n",
    "# Create \"short hand\" for the cost function to be minimized\n",
    "costFunction = lambda p: nnCostFunction(p, input_layer_size,\n",
    "                                        hidden_layer_size,\n",
    "                                        num_labels, X, y, lambda_)\n",
    "\n",
    "# Now, costFunction is a function that takes in only one argument\n",
    "# (the neural network parameters)\n",
    "res = optimize.minimize(costFunction,\n",
    "                        initial_nn_params,\n",
    "                        jac=True,\n",
    "                        method='TNC',\n",
    "                        options=options)\n",
    "\n",
    "# get the solution of the optimization\n",
    "nn_params = res.x\n",
    "        \n",
    "# Obtain Theta1 and Theta2 back from nn_params\n",
    "Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                    (hidden_layer_size, (input_layer_size + 1)))\n",
    "\n",
    "Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                    (num_labels, (hidden_layer_size + 1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f884d725dac834458d8894ad23ba7ab7d36539a91e65db527ec1cbe12b4486ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
